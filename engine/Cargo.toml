[package]
name = "openllm-server"
version = "0.1.0"
edition = "2021"
description = "OpenLLM inference engine with HTTP API and streaming support"

[dependencies]
axum = "0.7.5"
tokio = { version = "1.36.0", features = ["full"] }
serde = { version = "1.0.197", features = ["derive"] }
serde_json = "1.0.114"
tower = { version = "0.4.13", features = ["util", "timeout"] }
hyper = "1.2.0"
http = "1.0.0"
futures = "0.3.30"
tokio-stream = "0.1.15"
async-stream = "0.3.5"
tracing = "0.1.40"
tracing-subscriber = "0.3.18"
chrono = { version = "0.4.35", features = ["serde"] }
clap = { version = "4.4.18", features = ["derive"] }
uuid = { version = "1.6.1", features = ["v4", "serde"] }

[dev-dependencies]
reqwest = "0.12.3"

[profile.release]
opt-level = 3
lto = true
